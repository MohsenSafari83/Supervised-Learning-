# Comprehensive Guide to Performance Metrics

This module is a complete theoretical and practical guide for understanding and implementing key evaluation metrics for **classification models**.  
The main goal is to go beyond **Accuracy** and understand the complexities of model evaluation in **imbalanced datasets**.

---

## Main Objectives of This Guide

1. Understand the limitations of **Accuracy** in imbalanced problems.  
2. Master the basic outcome definitions (**TP, TN, FP, FN**).  
3. Learn how to implement and interpret the **Confusion Matrix**.  
4. Understand the trade-off between **Precision** and **Recall**.  
5. Learn how to implement performance metrics using **Scikit-learn** in Python.  

---

## Topics Covered

### 1. Basic Concepts

* **Accuracy:** definition, formula, and its limitations with real-world examples (e.g., cancer detection).  
* **Four Outcome Definitions:** detailed explanation of True Positive (TP), False Negative (FN), False Positive (FP), and True Negative (TN).  
* **Type I and Type II Errors:** understanding the difference between False Positive and False Negative.  

### 2. Core and Combined Metrics

* **Recall (Sensitivity):** importance in identifying actual positive cases.  
* **Precision:** importance in ensuring correctness of positive predictions.  
* **F1 Score:** understanding the harmonic mean and its role in balancing Precision and Recall.  

### 3. Advanced and Multi-class Evaluation

* **Confusion Matrix:** a visual and tabular tool for multi-class evaluation.  
* **Averaging Methods:** differences between **Micro-Averaging** and **Macro-Averaging**, and when to use each.  
* **AUC-ROC:** understanding the Receiver Operating Characteristic (ROC) curve and the **Area Under the Curve (AUC)** as a measure of a modelâ€™s discriminative ability.  



